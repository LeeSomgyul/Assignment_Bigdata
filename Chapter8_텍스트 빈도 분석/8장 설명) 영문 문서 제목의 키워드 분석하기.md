# [8장] 텍스트 빈도 분석: 영문 문서 제목의 키워드 분석하기
### 🖥️데이터 준비
#### ✅ In [ ]: 패키지 설치하기
    !pip install matplotlib
    !pip install wordcloud
1) matplotlib: 데이터를 그래프로 시각화 해주는 라이브러리
2) wordcloud: 데이터를 워드클라우드로 시각화 해주는 라이브러리

#### ✅ In [1]
    import pandas as pd
    import glob
    import re
    from functools import reduce
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    from collections import Counter
    import matplotlib.pyplot as plt
    from wordcloud import STOPWORDS, WordCloud
1) pandas: CSV, 엑셀 등의 데이터를 읽어서 테이블 형식으로 파일을 저장해주는 모듈
2) glob: 지정된 파일의 경로 및 이름의 패턴과 일치하는 모든 파일을 찾는 모듈
3) re: 문자열 검색 및 조작을 위한 라이브러리(ex. 특정 문자열에서 모든 이메일 주소 찾기)
4) reduce: 2차원 리스트를 1차원 리스트로 차원을 줄이기 위한 모듈
5) word_tokenize: 자연어 처리 패키지(nltk.tokenize)중에서 단어 토큰화 작업을 위한 모듈
6) stopwords: 자연어 처리 패키지(nltk.tokenize)중에서 불용어 정보를 제공하는 모듈
7) WordNetLemmatizer: 자연어 처리 패키지(nltk.tokenize)중에서 표제어 추출을 제공하는 모듈
8) Counter: 데이터 집합에서 개수를 자동으로 계산하기 위한 모듈
9) matplotlib.pyplot: 히스토그램으로 데이터 시각화를 위한 모듈
10) STOPWORDS, WordCloud: 워드클라우드를 그리기 위한 워드클라우드용 불용어 모듈과 그래프 생성 모듈

#### ✅ In [2]
    all_files = glob.glob('myCabinetExcelData*.xls')
    all_files
glob모듈을 이용하여 myCabinetExcelData로 시작하는 모든 파일을 all_files에 저장한다.

#### ☑️ Out [2]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/2c98c913-7d4c-40b6-8084-67c66a597e9e">

#### ✅ In [3]
    all_files_data = []
    for file in all_files:
        data_frame = pd.read_excel(file)
        all_files_data.append(data_frame)
    all_files_data[0]
all_files에 저장된 데이터들을 순서대로 읽어서 all_files_data배열에 추가(.append)한다.

#### ☑️ Out [3]
<img width="907" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/bcaeb2bd-d9be-47e3-a517-dc48034372a1">

###### 💡실행할 때 <img width="395" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/11092d7c-0a9b-4797-9538-46ba609756b8">와 같은 에러가 발생할 수 있다. 관련 모듈이 미설치되서 뜨는 에러로 아래 모듈을 설치한다.
    !pip install xlrd

#### ✅ In [4]
    all_files_data_concat = pd.concat(all_files_data, axis=0, ignore_index=True)
.concat함수로 데이터 프레임의 행 열을 병합하여 all_files_data_concat에 저장한다.

#### ☑️ Out [4]
<img width="931" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/3b5999ee-e1f1-4c97-b09e-e3472855af15">

#### ✅ In [5]
    all_files_data_concat.to_csv('riss_bigdata.csv', encoding = 'utf-8', index = False)
.to_csv함수로 병합된 all_files_data_concat을 CSV파일로 저장한다.

#### ✅ In [6]
    all_title = all_files_data_concat['제목']
all_files_data_concat에서 제목만 추출한다.

#### ☑️ Out [6]
<img width="450" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/fae2a2df-8c24-4810-a50f-c249a5ef97bf">

#### ✅ In [7]
    sotpWords = set(stopwords.words("english"))
    lemma = WordNetLemmatizer()
1) stopwords.words(): 영어 불용어를 추출한다.
2) WordNetLemmatizer(): 표제어 추출 작업을 제공하는 객체를 생성한다.

#### ✅ In [8]
    words = []
    
    for title in all_title:
        EnWords = re.sub(r"[^a-zA-Z]+", " ", str(title))
        EnWordsToken = word_tokenize(EnWords.lower())
        EnWordsTokenStop = [w for w in EnWordsToken if w not in stopwords]
        EnWordsTokenStopLemma = [lemma.lemmatize(w) for w in EnWordsTokenStop]
        words.append(EnWordsTokenStopLemma)
1) EnWords: 알파벳으로 시작하지 않는 단어("[^a-zA-Z]+")를 공백으로 치환하여 제거re.sub()한다.
2) EnWordsToken: EnWords를 소문자(.lower())로 정규화 후 단어 토큰화(word_tokenize())를 한다.
3) EnWordsTokenStop: EnWordsToken에서 불용어(stopwords)를 제거한다.
4) EnWordsTokenStopLemma: EnWordsTokenStop에서 표제어(.lemmatize())를 추출한다.
5) 마지막으로 EnWordsTokenStopLemma를 words 배열에 추가(.append)한다.

#### ✅ In [9]
    print(words)
wodrs 배열 내용을 출력한다.

###### 💡In [9]를 실행하면 아래와 같은 에러가 발생할 수 있다.
<img width="397" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/47946f3c-5842-4d55-9a85-357be6b352e3">

###### nltk.download('stopwords')오류 이외에도 여러가지가 뜰 수 있는데 필요한 기능이 부족해서 그런것이기 때문에 모두 다운받아주면 된다.
    import nltk
    nltk.download('stopwords') //불용어 목록 다운
    nltk.download('punkt') //토근화를 위한 데이터 다운
    nltk.download('wordnet') //텍스트 분석 작업에서 단어의 의미 파악 및 단어 간의 관계 이해를 위해 다운

###### 💡In [9]를 실행하면 <img width="450" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/146845ae-9118-4b38-a7ce-ca9390668cd1">와 같은 에러가 발생할 수 있다. 이는 stopwords 변수가 WordListCorpusReader 형식의 객체인데 In [8]의 for문을 통한 반복이 불가능하기 때문에 발생하는 에러이다. stopwords 변수에는 불용어 데이터가 포함된 객체가 아니라 실제 불용어 데이터를 포함하는 리스트가 필요하기 때문에 리스트로 변환 후 실행하면 정상 출력된다.
###### In [7]을 아래와 같이 수정한다.
    stopwords_list = stopwords.words('english')
    lemma = WordNetLemmatizer()
###### In [8]에서 EnWordsTokenStop변수 아래와 같이 수정한다.
    EnWordsTokenStop = [w for w in EnWordsToken if w not in stopwords_list]

#### ☑️ Out [9]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/1d13afbb-0244-48e2-946a-64d03b3708ad">

#### ✅ In [10]
    words2 = list(reduce(lambda x, y: x+y, words))
    print(words2)
reduce()를 사용하여 words를 1차원 배열로 변환한다.

#### ☑️ Out [10]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/40d57d42-07e5-4642-8c4b-2f7d34ad485c">


### 🖥️데이터 탐색 및 분석 모델 구축
#### ✅ In [11]
    count = Counter(words2)
    print(count)
Counter()를 사용하여 words2 리스트에 있는 단어별로 출현 횟수를 계산한다.

#### ☑️ Out [11]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/9ebf2cd6-f188-4847-a3ed-f0307085659b">

#### ✅ In [12]
    word_count = dict()
    
    for tag, counts in count.most_common(50):
        if(len(str(tag))>1):
            word_count[tag] = counts
            print("%s : %d" % (tag, counts))
1) count.most_common(50): count에서 가장 많이 출현한 상위 50개 단어를 리스트로 반환한다.
2) tag, counts: 반복문을 통해 1)의 결과를 tag에는 단어를 counts에는 해당 단어(tag)의 출현 횟수를 저장한다.
3) len(str(tag))>1: 단어(tag)의 길이가 1이상인 것만 다음 함수를 실행한다.
4) word_count[tag] = counts: 상단의 word_count 딕셔너리(dict())에 각 tag(단어)와 counts(등장 횟수)를 저장한다.

#### ☑️ Out [12]
<img width="200" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/98cf62ec-4c11-4854-b53c-1a3a966ba4db">

#### ✅ In [13]
    sorted_Keys = sorted(word_count, key = word_count.get, reverse=True)
    sorted_Values = sorted(word_count.values(), reverse=True)
    plt.bar(range(len(word_count)), sorted_Values, align='center')
    plt.xticks(range(len(word_count)), list(sorted_Keys), rotation = 85)
    plt.show()
1) sorted_Keys: 딕셔너리의 키를 값에 따라 내림차순으로 정렬한다.
2) sorted_Values: 딕셔너리의 값을 내림차순으로 정렬한다.
3) plt.bar(): word_count의 길이만큼 x축 범위를 설정하고, sorted_Values를 y축 값으로 사용하여 막대 그래프를 그린다.
4) plt.xticks(): x축의 눈금 값들을 sorted_Keys로 하고 85도 회전시켜 나타낸다.
###### 💡교과서에는 rotation = '85'로 되어있지만 <img width="397" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/32e001a6-7677-4830-9267-90d22a96dddf">에러가 발생한다면 rotation = 85로 수정하면 해결된다.

#### ☑️ Out [13]
<img width="450" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/9703d4f5-794f-4a1f-8689-5adddf8134de">


### 🖥️결과 시각화
#### ✅ In [14]
    all_files_data_concat['doc_count'] = 0
    summary_year = all_files_data_concat.groupby('출판일', as_index = False)['doc_count'].count()
1) all_files_data_concat에 'doc_count' 컬럼을 추가한다.
2) summary_year: '출판일'을 기준으로 그룹을 만들어서(.groupby()) 그룹별 데이터 개수(.count())를 'doc_count'컬럼에 저장하는 리스트를 생성한다.

#### ☑️ Out [14]
<img width="250" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/7833cdc0-cc6c-4eb4-8d59-ece73c1557da">

#### ✅ In [15]
    plt.figure(figsize=(12, 5))
    plt.xlabel("year")
    plt.ylabel("doc-count")
    plt.grid(True)
    plt.plot(range(len(summary_year)), summary_year['doc_count'])
    plt.xticks(range(len(summary_year)), [text for text in summary_year['출판일']])
    plt.show()
1) .figure(): 너비 12인치, 높이 5인치의 새로운 그래프를 생성한다.
2) .xlabel(): x축 이름을 'year'로 한다.
3) .ylabel(): y축 이름을 'doc_count'로 한다.
4) .grid(): 그래프에 눈금을 추가한다.
5) .plot(): x축에 들어갈 값의 개수(길이)를 summary_year의 배열 개수만큼으로 지정하고, y축의 값은 'doc_count'컬럼으로 한다.
6) .xticks(): x축의 값은 '출판일'로 한다.

#### ☑️ Out [15]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/7fc934e7-21ca-49c0-982b-67b3451aa968">

#### ✅ In [16]
    stopwords = set(STOPWORDS)
    wc = WordCloud(background_color = 'ivory', stopwords = stopwords, width = 800, height = 600)
    cloud = wc.generate_from_frequencies(word_count)
    plt.figure(figsize=(8, 8))
    plt.imshow(cloud)
    plt.axis('off')
    plt.show()
1) stopwords: 워드클라우드에서 사용할 불용어를 설정한다.
2) wc: 워드클라우드의 외관을 설정한다.
3) cloud: generate_from_frequencies()로 각 닥어의 빈도수에 따른 워드클라우드를 생성한다.
4) .figure(): 새로운 그림을 가로8 세로8 사이즈로 생성한다.
5) .imshow(): cloud에 저장된 이미지를 출력한다.
6) .axis(): 워드클라우드에는 축이 필요하지 않기 때문에 축을 숨긴다.

#### ☑️ Out [16]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/3bf12276-3156-4cbd-87e2-bc42bb964994">

#### ✅ In [17]
    cloud.to_file("riss.bigdata_wordCloud.jpg")
생성한 워드클라우드를 jpg파일로 저장합니다.
