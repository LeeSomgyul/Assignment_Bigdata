# [8장] 텍스트 빈도 분석: 영문 문서 제목의 키워드 분석하기
### 🖥️데이터 준비
#### ✅ In [ ]: 패키지 설치하기
    !pip install matplotlib
    !pip install wordcloud
1) matplotlib: 데이터를 그래프로 시각화 해주는 라이브러리
2) wordcloud: 데이터를 워드클라우드로 시각화 해주는 라이브러리

#### ✅ In [1]
    import pandas as pd
    import glob
    import re
    from functools import reduce
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    from collections import Counter
    import matplotlib.pyplot as plt
    from wordcloud import STOPWORDS, WordCloud
1) pandas: CSV, 엑셀 등의 데이터를 읽어서 테이블 형식으로 파일을 저장해주는 모듈
2) glob: 지정된 파일의 경로 및 이름의 패턴과 일치하는 모든 파일을 찾는 모듈
3) re: 문자열 검색 및 조작을 위한 라이브러리(ex. 특정 문자열에서 모든 이메일 주소 찾기)
4) reduce: 2차원 리스트를 1차원 리스트로 차원을 줄이기 위한 모듈
5) word_tokenize: 자연어 처리 패키지(nltk.tokenize)중에서 단어 토큰화 작업을 위한 모듈
6) stopwords: 자연어 처리 패키지(nltk.tokenize)중에서 불용어 정보를 제공하는 모듈
7) WordNetLemmatizer: 자연어 처리 패키지(nltk.tokenize)중에서 표제어 추출을 제공하는 모듈
8) Counter: 데이터 집합에서 개수를 자동으로 계산하기 위한 모듈
9) matplotlib.pyplot: 히스토그램으로 데이터 시각화를 위한 모듈
10) STOPWORDS, WordCloud: 워드클라우드를 그리기 위한 워드클라우드용 불용어 모듈과 그래프 생성 모듈

#### ✅ In [2]
    all_files = glob.glob('myCabinetExcelData*.xls')
    all_files
glob모듈을 이용하여 myCabinetExcelData로 시작하는 모든 파일을 all_files에 저장한다.

#### ☑️ Out [2]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/2c98c913-7d4c-40b6-8084-67c66a597e9e">

#### ✅ In [3]
    all_files_data = []
    for file in all_files:
        data_frame = pd.read_excel(file)
        all_files_data.append(data_frame)
    all_files_data[0]
all_files에 저장된 데이터들을 순서대로 읽어서 all_files_data배열에 추가(.append)한다.

#### ☑️ Out [3]
<img width="907" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/bcaeb2bd-d9be-47e3-a517-dc48034372a1">

###### 💡실행할 때 <img width="395" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/11092d7c-0a9b-4797-9538-46ba609756b8">와 같은 에러가 발생할 수 있다. 관련 모듈이 미설치되서 뜨는 에러로 아래 모듈을 설치한다.
    !pip install xlrd

#### ✅ In [4]
    all_files_data_concat = pd.concat(all_files_data, axis=0, ignore_index=True)
.concat함수로 데이터 프레임의 행 열을 병합하여 all_files_data_concat에 저장한다.

#### ☑️ Out [4]
<img width="931" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/3b5999ee-e1f1-4c97-b09e-e3472855af15">

#### ✅ In [5]
    all_files_data_concat.to_csv('riss_bigdata.csv', encoding = 'utf-8', index = False)
.to_csv함수로 병합된 all_files_data_concat을 CSV파일로 저장한다.

#### ✅ In [6]
    all_title = all_files_data_concat['제목']
all_files_data_concat에서 제목만 추출한다.

#### ☑️ Out [6]
<img width="450" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/fae2a2df-8c24-4810-a50f-c249a5ef97bf">

#### ✅ In [7]
    sotpWords = set(stopwords.words("english"))
    lemma = WordNetLemmatizer()
1) stopwords.words(): 영어 불용어를 추출한다.
2) WordNetLemmatizer(): 표제어 추출 작업을 제공하는 객체를 생성한다.

#### ✅ In [8]
    words = []
    
    for title in all_title:
        EnWords = re.sub(r"[^a-zA-Z]+", " ", str(title))
        EnWordsToken = word_tokenize(EnWords.lower())
        EnWordsTokenStop = [w for w in EnWordsToken if w not in stopwords]
        EnWordsTokenStopLemma = [lemma.lemmatize(w) for w in EnWordsTokenStop]
        words.append(EnWordsTokenStopLemma)
1) EnWords: 알파벳으로 시작하지 않는 단어("[^a-zA-Z]+")를 공백으로 치환하여 제거re.sub()한다.
2) EnWordsToken: EnWords를 소문자(.lower())로 정규화 후 단어 토큰화(word_tokenize())를 한다.
3) EnWordsTokenStop: EnWordsToken에서 불용어(stopwords)를 제거한다.
4) EnWordsTokenStopLemma: EnWordsTokenStop에서 표제어(.lemmatize())를 추출한다.
5) 마지막으로 EnWordsTokenStopLemma를 words 배열에 추가(.append)한다.

#### ✅ In [9]
    print(words)
wodrs 배열 내용을 출력한다.

###### 💡In [9]를 실행하면 아래와 같은 에러가 뜰 수 있다.
<img width="397" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/47946f3c-5842-4d55-9a85-357be6b352e3">

###### nltk.download('stopwords')오류 이외에도 여러가지가 뜰 수 있는데 필요한 기능이 부족해서 그런것이기 때문에 모두 다운받아주면 된다.
    import nltk
    nltk.download('stopwords') //불용어 목록 다운
    nltk.download('punkt') //토근화를 위한 데이터 다운
    nltk.download('wordnet') //텍스트 분석 작업에서 단어의 의미 파악 및 단어 간의 관계 이해를 위해 다운
