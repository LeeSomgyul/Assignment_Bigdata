# [8ì¥] í…ìŠ¤íŠ¸ ë¹ˆë„ ë¶„ì„: ì˜ë¬¸ ë¬¸ì„œ ì œëª©ì˜ í‚¤ì›Œë“œ ë¶„ì„í•˜ê¸°
### ğŸ–¥ï¸ë°ì´í„° ì¤€ë¹„
#### âœ… In [ ]: íŒ¨í‚¤ì§€ ì„¤ì¹˜í•˜ê¸°
    !pip install matplotlib
    !pip install wordcloud
1) matplotlib: ë°ì´í„°ë¥¼ ê·¸ë˜í”„ë¡œ ì‹œê°í™” í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
2) wordcloud: ë°ì´í„°ë¥¼ ì›Œë“œí´ë¼ìš°ë“œë¡œ ì‹œê°í™” í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬

#### âœ… In [1]
    import pandas as pd
    import glob
    import re
    from functools import reduce
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    from collections import Counter
    import matplotlib.pyplot as plt
    from wordcloud import STOPWORDS, WordCloud
1) pandas: CSV, ì—‘ì…€ ë“±ì˜ ë°ì´í„°ë¥¼ ì½ì–´ì„œ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ íŒŒì¼ì„ ì €ì¥í•´ì£¼ëŠ” ëª¨ë“ˆ
2) glob: ì§€ì •ëœ íŒŒì¼ì˜ ê²½ë¡œ ë° ì´ë¦„ì˜ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  íŒŒì¼ì„ ì°¾ëŠ” ëª¨ë“ˆ
3) re: ë¬¸ìì—´ ê²€ìƒ‰ ë° ì¡°ì‘ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(ex. íŠ¹ì • ë¬¸ìì—´ì—ì„œ ëª¨ë“  ì´ë©”ì¼ ì£¼ì†Œ ì°¾ê¸°)
4) reduce: 2ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¥¼ 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¡œ ì°¨ì›ì„ ì¤„ì´ê¸° ìœ„í•œ ëª¨ë“ˆ
5) word_tokenize: ìì—°ì–´ ì²˜ë¦¬ íŒ¨í‚¤ì§€(nltk.tokenize)ì¤‘ì—ì„œ ë‹¨ì–´ í† í°í™” ì‘ì—…ì„ ìœ„í•œ ëª¨ë“ˆ
6) stopwords: ìì—°ì–´ ì²˜ë¦¬ íŒ¨í‚¤ì§€(nltk.tokenize)ì¤‘ì—ì„œ ë¶ˆìš©ì–´ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ëª¨ë“ˆ
7) WordNetLemmatizer: ìì—°ì–´ ì²˜ë¦¬ íŒ¨í‚¤ì§€(nltk.tokenize)ì¤‘ì—ì„œ í‘œì œì–´ ì¶”ì¶œì„ ì œê³µí•˜ëŠ” ëª¨ë“ˆ
8) Counter: ë°ì´í„° ì§‘í•©ì—ì„œ ê°œìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ìœ„í•œ ëª¨ë“ˆ
9) matplotlib.pyplot: íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ë°ì´í„° ì‹œê°í™”ë¥¼ ìœ„í•œ ëª¨ë“ˆ
10) STOPWORDS, WordCloud: ì›Œë“œí´ë¼ìš°ë“œë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ ì›Œë“œí´ë¼ìš°ë“œìš© ë¶ˆìš©ì–´ ëª¨ë“ˆê³¼ ê·¸ë˜í”„ ìƒì„± ëª¨ë“ˆ

#### âœ… In [2]
    all_files = glob.glob('myCabinetExcelData*.xls')
    all_files
globëª¨ë“ˆì„ ì´ìš©í•˜ì—¬ myCabinetExcelDataë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  íŒŒì¼ì„ all_filesì— ì €ì¥í•œë‹¤.

#### â˜‘ï¸ Out [2]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/2c98c913-7d4c-40b6-8084-67c66a597e9e">

#### âœ… In [3]
    all_files_data = []
    for file in all_files:
        data_frame = pd.read_excel(file)
        all_files_data.append(data_frame)
    all_files_data[0]
all_filesì— ì €ì¥ëœ ë°ì´í„°ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì½ì–´ì„œ all_files_dataë°°ì—´ì— ì¶”ê°€(.append)í•œë‹¤.

#### â˜‘ï¸ Out [3]
<img width="907" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/bcaeb2bd-d9be-47e3-a517-dc48034372a1">

###### ğŸ’¡ì‹¤í–‰í•  ë•Œ <img width="395" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/11092d7c-0a9b-4797-9538-46ba609756b8">ì™€ ê°™ì€ ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ê´€ë ¨ ëª¨ë“ˆì´ ë¯¸ì„¤ì¹˜ë˜ì„œ ëœ¨ëŠ” ì—ëŸ¬ë¡œ ì•„ë˜ ëª¨ë“ˆì„ ì„¤ì¹˜í•œë‹¤.
    !pip install xlrd

#### âœ… In [4]
    all_files_data_concat = pd.concat(all_files_data, axis=0, ignore_index=True)
.concatí•¨ìˆ˜ë¡œ ë°ì´í„° í”„ë ˆì„ì˜ í–‰ ì—´ì„ ë³‘í•©í•˜ì—¬ all_files_data_concatì— ì €ì¥í•œë‹¤.

#### â˜‘ï¸ Out [4]
<img width="931" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/3b5999ee-e1f1-4c97-b09e-e3472855af15">

#### âœ… In [5]
    all_files_data_concat.to_csv('riss_bigdata.csv', encoding = 'utf-8', index = False)
.to_csví•¨ìˆ˜ë¡œ ë³‘í•©ëœ all_files_data_concatì„ CSVíŒŒì¼ë¡œ ì €ì¥í•œë‹¤.

#### âœ… In [6]
    all_title = all_files_data_concat['ì œëª©']
all_files_data_concatì—ì„œ ì œëª©ë§Œ ì¶”ì¶œí•œë‹¤.

#### â˜‘ï¸ Out [6]
<img width="450" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/fae2a2df-8c24-4810-a50f-c249a5ef97bf">

#### âœ… In [7]
    sotpWords = set(stopwords.words("english"))
    lemma = WordNetLemmatizer()
1) stopwords.words(): ì˜ì–´ ë¶ˆìš©ì–´ë¥¼ ì¶”ì¶œí•œë‹¤.
2) WordNetLemmatizer(): í‘œì œì–´ ì¶”ì¶œ ì‘ì—…ì„ ì œê³µí•˜ëŠ” ê°ì²´ë¥¼ ìƒì„±í•œë‹¤.

#### âœ… In [8]
    words = []
    
    for title in all_title:
        EnWords = re.sub(r"[^a-zA-Z]+", " ", str(title))
        EnWordsToken = word_tokenize(EnWords.lower())
        EnWordsTokenStop = [w for w in EnWordsToken if w not in stopwords]
        EnWordsTokenStopLemma = [lemma.lemmatize(w) for w in EnWordsTokenStop]
        words.append(EnWordsTokenStopLemma)
1) EnWords: ì•ŒíŒŒë²³ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´("[^a-zA-Z]+")ë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ ì œê±°re.sub()í•œë‹¤.
2) EnWordsToken: EnWordsë¥¼ ì†Œë¬¸ì(.lower())ë¡œ ì •ê·œí™” í›„ ë‹¨ì–´ í† í°í™”(word_tokenize())ë¥¼ í•œë‹¤.
3) EnWordsTokenStop: EnWordsTokenì—ì„œ ë¶ˆìš©ì–´(stopwords)ë¥¼ ì œê±°í•œë‹¤.
4) EnWordsTokenStopLemma: EnWordsTokenStopì—ì„œ í‘œì œì–´(.lemmatize())ë¥¼ ì¶”ì¶œí•œë‹¤.
5) ë§ˆì§€ë§‰ìœ¼ë¡œ EnWordsTokenStopLemmaë¥¼ words ë°°ì—´ì— ì¶”ê°€(.append)í•œë‹¤.

#### âœ… In [9]
    print(words)
wodrs ë°°ì—´ ë‚´ìš©ì„ ì¶œë ¥í•œë‹¤.

###### ğŸ’¡In [9]ë¥¼ ì‹¤í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
<img width="397" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/47946f3c-5842-4d55-9a85-357be6b352e3">

###### nltk.download('stopwords')ì˜¤ë¥˜ ì´ì™¸ì—ë„ ì—¬ëŸ¬ê°€ì§€ê°€ ëœ° ìˆ˜ ìˆëŠ”ë° í•„ìš”í•œ ê¸°ëŠ¥ì´ ë¶€ì¡±í•´ì„œ ê·¸ëŸ°ê²ƒì´ê¸° ë•Œë¬¸ì— ëª¨ë‘ ë‹¤ìš´ë°›ì•„ì£¼ë©´ ëœë‹¤.
    import nltk
    nltk.download('stopwords') //ë¶ˆìš©ì–´ ëª©ë¡ ë‹¤ìš´
    nltk.download('punkt') //í† ê·¼í™”ë¥¼ ìœ„í•œ ë°ì´í„° ë‹¤ìš´
    nltk.download('wordnet') //í…ìŠ¤íŠ¸ ë¶„ì„ ì‘ì—…ì—ì„œ ë‹¨ì–´ì˜ ì˜ë¯¸ íŒŒì•… ë° ë‹¨ì–´ ê°„ì˜ ê´€ê³„ ì´í•´ë¥¼ ìœ„í•´ ë‹¤ìš´

###### ğŸ’¡In [9]ë¥¼ ì‹¤í–‰í•˜ë©´ <img width="450" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/146845ae-9118-4b38-a7ce-ca9390668cd1">ì™€ ê°™ì€ ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ì´ëŠ” stopwords ë³€ìˆ˜ê°€ WordListCorpusReader í˜•ì‹ì˜ ê°ì²´ì¸ë° In [8]ì˜ forë¬¸ì„ í†µí•œ ë°˜ë³µì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ì—ëŸ¬ì´ë‹¤. stopwords ë³€ìˆ˜ì—ëŠ” ë¶ˆìš©ì–´ ë°ì´í„°ê°€ í¬í•¨ëœ ê°ì²´ê°€ ì•„ë‹ˆë¼ ì‹¤ì œ ë¶ˆìš©ì–´ ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ì‹¤í–‰í•˜ë©´ ì •ìƒ ì¶œë ¥ëœë‹¤.
###### In [7]ì„ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì •í•œë‹¤.
    stopwords_list = stopwords.words('english')
    lemma = WordNetLemmatizer()
###### In [8]ì—ì„œ EnWordsTokenStopë³€ìˆ˜ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì •í•œë‹¤.
    EnWordsTokenStop = [w for w in EnWordsToken if w not in stopwords_list]

#### â˜‘ï¸ Out [9]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/1d13afbb-0244-48e2-946a-64d03b3708ad">

#### âœ… In [10]
    words2 = list(reduce(lambda x, y: x+y, words))
    print(words2)
reduce()ë¥¼ ì‚¬ìš©í•˜ì—¬ wordsë¥¼ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜í•œë‹¤.

#### â˜‘ï¸ Out [10]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/40d57d42-07e5-4642-8c4b-2f7d34ad485c">


### ğŸ–¥ï¸ë°ì´í„° íƒìƒ‰ ë° ë¶„ì„ ëª¨ë¸ êµ¬ì¶•
#### âœ… In [11]
    count = Counter(words2)
    print(count)
Counter()ë¥¼ ì‚¬ìš©í•˜ì—¬ words2 ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë‹¨ì–´ë³„ë¡œ ì¶œí˜„ íšŸìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤.

#### â˜‘ï¸ Out [11]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/9ebf2cd6-f188-4847-a3ed-f0307085659b">

#### âœ… In [12]
    word_count = dict()
    
    for tag, counts in count.most_common(50):
        if(len(str(tag))>1):
            word_count[tag] = counts
            print("%s : %d" % (tag, counts))
1) count.most_common(50): countì—ì„œ ê°€ì¥ ë§ì´ ì¶œí˜„í•œ ìƒìœ„ 50ê°œ ë‹¨ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•œë‹¤.
2) tag, counts: ë°˜ë³µë¬¸ì„ í†µí•´ 1)ì˜ ê²°ê³¼ë¥¼ tagì—ëŠ” ë‹¨ì–´ë¥¼ countsì—ëŠ” í•´ë‹¹ ë‹¨ì–´(tag)ì˜ ì¶œí˜„ íšŸìˆ˜ë¥¼ ì €ì¥í•œë‹¤.
3) len(str(tag))>1: ë‹¨ì–´(tag)ì˜ ê¸¸ì´ê°€ 1ì´ìƒì¸ ê²ƒë§Œ ë‹¤ìŒ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•œë‹¤.
4) word_count[tag] = counts: ìƒë‹¨ì˜ word_count ë”•ì…”ë„ˆë¦¬(dict())ì— ê° tag(ë‹¨ì–´)ì™€ counts(ë“±ì¥ íšŸìˆ˜)ë¥¼ ì €ì¥í•œë‹¤.

#### â˜‘ï¸ Out [12]
<img width="200" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/98cf62ec-4c11-4854-b53c-1a3a966ba4db">

#### âœ… In [13]
    sorted_Keys = sorted(word_count, key = word_count.get, reverse=True)
    sorted_Values = sorted(word_count.values(), reverse=True)
    plt.bar(range(len(word_count)), sorted_Values, align='center')
    plt.xticks(range(len(word_count)), list(sorted_Keys), rotation = 85)
    plt.show()
1) sorted_Keys: ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ë¥¼ ê°’ì— ë”°ë¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•œë‹¤.
2) sorted_Values: ë”•ì…”ë„ˆë¦¬ì˜ ê°’ì„ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•œë‹¤.
3) plt.bar(): word_countì˜ ê¸¸ì´ë§Œí¼ xì¶• ë²”ìœ„ë¥¼ ì„¤ì •í•˜ê³ , sorted_Valuesë¥¼ yì¶• ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë§‰ëŒ€ ê·¸ë˜í”„ë¥¼ ê·¸ë¦°ë‹¤.
4) plt.xticks(): xì¶•ì˜ ëˆˆê¸ˆ ê°’ë“¤ì„ sorted_Keysë¡œ í•˜ê³  85ë„ íšŒì „ì‹œì¼œ ë‚˜íƒ€ë‚¸ë‹¤.
###### ğŸ’¡êµê³¼ì„œì—ëŠ” rotation = '85'ë¡œ ë˜ì–´ìˆì§€ë§Œ <img width="397" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/32e001a6-7677-4830-9267-90d22a96dddf">ì—ëŸ¬ê°€ ë°œìƒí•œë‹¤ë©´ rotation = 85ë¡œ ìˆ˜ì •í•˜ë©´ í•´ê²°ëœë‹¤.

#### â˜‘ï¸ Out [13]
<img width="450" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/9703d4f5-794f-4a1f-8689-5adddf8134de">


### ğŸ–¥ï¸ê²°ê³¼ ì‹œê°í™”
#### âœ… In [14]
    all_files_data_concat['doc_count'] = 0
    summary_year = all_files_data_concat.groupby('ì¶œíŒì¼', as_index = False)['doc_count'].count()
1) all_files_data_concatì— 'doc_count' ì»¬ëŸ¼ì„ ì¶”ê°€í•œë‹¤.
2) summary_year: 'ì¶œíŒì¼'ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹ì„ ë§Œë“¤ì–´ì„œ(.groupby()) ê·¸ë£¹ë³„ ë°ì´í„° ê°œìˆ˜(.count())ë¥¼ 'doc_count'ì»¬ëŸ¼ì— ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤.

#### â˜‘ï¸ Out [14]
<img width="250" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/7833cdc0-cc6c-4eb4-8d59-ece73c1557da">

#### âœ… In [15]
    plt.figure(figsize=(12, 5))
    plt.xlabel("year")
    plt.ylabel("doc-count")
    plt.grid(True)
    plt.plot(range(len(summary_year)), summary_year['doc_count'])
    plt.xticks(range(len(summary_year)), [text for text in summary_year['ì¶œíŒì¼']])
    plt.show()
1) .figure(): ë„ˆë¹„ 12ì¸ì¹˜, ë†’ì´ 5ì¸ì¹˜ì˜ ìƒˆë¡œìš´ ê·¸ë˜í”„ë¥¼ ìƒì„±í•œë‹¤.
2) .xlabel(): xì¶• ì´ë¦„ì„ 'year'ë¡œ í•œë‹¤.
3) .ylabel(): yì¶• ì´ë¦„ì„ 'doc_count'ë¡œ í•œë‹¤.
4) .grid(): ê·¸ë˜í”„ì— ëˆˆê¸ˆì„ ì¶”ê°€í•œë‹¤.
5) .plot(): xì¶•ì— ë“¤ì–´ê°ˆ ê°’ì˜ ê°œìˆ˜(ê¸¸ì´)ë¥¼ summary_yearì˜ ë°°ì—´ ê°œìˆ˜ë§Œí¼ìœ¼ë¡œ ì§€ì •í•˜ê³ , yì¶•ì˜ ê°’ì€ 'doc_count'ì»¬ëŸ¼ìœ¼ë¡œ í•œë‹¤.
6) .xticks(): xì¶•ì˜ ê°’ì€ 'ì¶œíŒì¼'ë¡œ í•œë‹¤.

#### â˜‘ï¸ Out [15]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/7fc934e7-21ca-49c0-982b-67b3451aa968">

#### âœ… In [16]
    stopwords = set(STOPWORDS)
    wc = WordCloud(background_color = 'ivory', stopwords = stopwords, width = 800, height = 600)
    cloud = wc.generate_from_frequencies(word_count)
    plt.figure(figsize=(8, 8))
    plt.imshow(cloud)
    plt.axis('off')
    plt.show()
1) stopwords: ì›Œë“œí´ë¼ìš°ë“œì—ì„œ ì‚¬ìš©í•  ë¶ˆìš©ì–´ë¥¼ ì„¤ì •í•œë‹¤.
2) wc: ì›Œë“œí´ë¼ìš°ë“œì˜ ì™¸ê´€ì„ ì„¤ì •í•œë‹¤.
3) cloud: generate_from_frequencies()ë¡œ ê° ë‹¥ì–´ì˜ ë¹ˆë„ìˆ˜ì— ë”°ë¥¸ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•œë‹¤.
4) .figure(): ìƒˆë¡œìš´ ê·¸ë¦¼ì„ ê°€ë¡œ8 ì„¸ë¡œ8 ì‚¬ì´ì¦ˆë¡œ ìƒì„±í•œë‹¤.
5) .imshow(): cloudì— ì €ì¥ëœ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•œë‹¤.
6) .axis(): ì›Œë“œí´ë¼ìš°ë“œì—ëŠ” ì¶•ì´ í•„ìš”í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì¶•ì„ ìˆ¨ê¸´ë‹¤.

#### â˜‘ï¸ Out [16]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/3bf12276-3156-4cbd-87e2-bc42bb964994">

#### âœ… In [17]
    cloud.to_file("riss.bigdata_wordCloud.jpg")
ìƒì„±í•œ ì›Œë“œí´ë¼ìš°ë“œë¥¼ jpgíŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
