# [8장] 텍스트 빈도 분석: 한글 뉴스 기사의 키워드 분석하기
### 🖥️데이터 준비
#### ✅ In [1]
    import json
    import re
    from konlpy.tag import Okt
    from collections import Counter
    import matplotlib
    import matplotlib.pyplot as plt
    from matplotlib import font_manager, rc
    from wordcloud import WordCloud
1) json: json형식의 데이터를 다루는데 사용하는 모듈
2) re: 문자열 검색 및 조작을 위한 모듈(ex. 특정 문자열에서 모든 이메일 주소 찾기)
3) konlpy.tag.Okt: 한국어 자연어 처리를 위한 모듈로 한국어 문장을 토큰 단위로 분석할 수 있게 해준다.
4) collections.Counter: 객체 요소들의 개수를 세어 딕셔너리 형태로 변환해주는 모듈
5) matplotlib: 데이터를 시각화 하기 위한 모듈
6) matplotlib.pyplot: matplotlib의 디자인을 위한 모듈
7) matplotlib.rc: matplotlib의 폰트, 색상 등을 변경할 수 있는 모듈
8) wordcloud.WordCloud: 텍스트 데이터 빈도수를 이용하여 워드클라우드를 생성하는 모듈

#### ✅ In [2]
  inputFileName = 'etnews.kr_facebook_2016-01-01_2018-08-01_4차 산업혁명'
  data = json.loads(open(inputFileName+'.json', 'r', encoding = 'utf-8').read())
수집한 데이터파일을 열어서 json형식으로 data변수에 저장한다.

#### ☑️ Out [2]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/e60bc2ab-0889-4e8b-be5d-990028973efc">

#### ✅ In [3]
    message = ''
    
    for item in data:
        if 'message' in item.keys():
            message = message + re.sub(r'[^\w]', ' ', item['message'])+''
뉴스 본문 내용(message)에서 문자나 숫자가 아닌 것(r'[^\w]')은 공백으로 바꿔(re.sub()) 제거한 뒤 연결하여 내용 전체를 하나의 문자열로 message에 저장한다.

#### ☑️ Out [3]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/b44784ae-9612-4134-a983-574916d7c7e0">

#### ✅ In [4]
    nlp = Okt()
    message_N = nlp.nouns(message)
한국어 자연어 처리를 위한 Okt()객체를 생성한 뒤 nlp.nouns()를 통하여 message에서 명사를 추출한다.

#### ☑️ Out [4]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/b20a4926-e278-4507-995c-bd47b9e10d80">


### 🖥️데이터 탐색 및 분석 모델 구축
#### ✅ In [5]
    count = Counter(message_N)
각 단어별 출현 횟수를 계산한다.

#### ☑️ Out [5]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/b10ff67d-7feb-470e-b848-c0409a28c07f">

#### ✅ In [6]
    word_count = dict()
    for tag, counts in count.most_common(80):
        if(len(str(tag))>1):
            word_count[tag] = counts
            print("%s : %d" % (tag, counts))
출현 횟수가 많은 상위 80개의 단어 중에서 길이가 1보다 큰 것만 word_count 딕셔너리에 저장하며, 키는 tag 값은 counts로 한다.

#### ☑️ Out [6]
<img width="180" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/4a891027-aa13-48ea-adb6-70ebc9e6e133">

#### ✅ In [7]
    font_path = "c:/Windows/fonts/malgun.ttf"
    font_name = font_manager.FontProperties(fname = font_path).get_name()
    matplotlib.rc('font', family = font_name)
히스토그램에서 한글로 표시하기 위해 맑은고딕 폰트를 설정한다.

#### ✅ In [8]
    plt.figure(figsize=(12, 5))
    plt.xlabel('키워드')
    plt.ylabel('빈도수')
    plt.grid(True)
    sorted_Keys = sorted(word_count, key = word_count.get, reverse = True)
    sorted_Values = sorted(word_count.values(), reverse = True)
    plt.bar(range(len(word_count)), sorted_Values, align = 'center')
    plt.xticks(range(len(word_count)), list(sorted_Keys), rotation = 75)
    plt.show()
x축을 '키워드', y축을 '빈도수'로 하여 히스토그램을 제작한다.
자세한 방법은 '8장 설명) 영문 문서 제목의 키워드 분석하기.md'와 동일.

#### ☑️ Out [8]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/1f753e60-26d9-45e7-9932-f9b1a5ba604e">


### 🖥️결과 시각화
#### ✅ In [9]
    wc = WordCloud(font_path, background_color='ivory', width=800, height=600)
    cloud = wc.generate_from_frequencies(word_count)
    plt.figure(figsize=(8, 8))
    plt.imshow(cloud)
    plt.axis('off')
    plt.show()
단어별 빈도수를 계산해서(.generate_from_frequencies()) 워드클라우드를 생성한다.

#### ☑️ Out [9]
<img width="450" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/40b28df6-d8f2-4da7-9cd9-44b5aa089a8d">

#### ✅ In [10]
    cloud.to_file(inputFileName + '_cloud.jpg')
워드클라우드를 jpg 파일로 저장한다.
