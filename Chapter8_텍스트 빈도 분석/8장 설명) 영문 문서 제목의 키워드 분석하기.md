# [8장] 텍스트 빈도 분석: 영문 문서 제목의 키워드 분석하기
### 🖥️데이터 준비
#### ✅ In [ ]: 패키지 설치하기
    !pip install matplotlib
    !pip install wordcloud
1) matplotlib: 데이터를 그래프로 시각화 해주는 라이브러리
2) wordcloud: 데이터를 워드클라우드로 시각화 해주는 라이브러리

#### ✅ In [1]
    import pandas as pd
    import glob
    import re
    from functools import reduce
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    from collections import Counter
    import matplotlib.pyplot as plt
    from wordcloud import STOPWORDS, WordCloud
1) pandas: CSV, 엑셀 등의 데이터를 읽어서 테이블 형식으로 파일을 저장해주는 모듈
2) glob: 지정된 파일의 경로 및 이름의 패턴과 일치하는 모든 파일을 찾는 모듈
3) re: 문자열 검색 및 조작을 위한 라이브러리(ex. 특정 문자열에서 모든 이메일 주소 찾기)
4) reduce: 2차원 리스트를 1차원 리스트로 차원을 줄이기 위한 모듈
5) word_tokenize: 자연어 처리 패키지(nltk.tokenize)중에서 단어 토큰화 작업을 위한 모듈
6) stopwords: 자연어 처리 패키지(nltk.tokenize)중에서 불용어 정보를 제공하는 모듈
7) WordNetLemmatizer: 자연어 처리 패키지(nltk.tokenize)중에서 표제어 추출을 제공하는 모듈
8) Counter: 데이터 집합에서 개수를 자동으로 계산하기 위한 모듈
9) matplotlib.pyplot: 히스토그램으로 데이터 시각화를 위한 모듈
10) STOPWORDS, WordCloud: 워드클라우드를 그리기 위한 워드클라우드용 불용어 모듈과 그래프 생성 모듈

#### ✅ In [2]
    all_files = glob.glob('myCabinetExcelData*.xls')
    all_files
glob모듈을 이용하여 myCabinetExcelData로 시작하는 모든 파일을 all_files에 저장한다.

#### ☑️ Out [2]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/2c98c913-7d4c-40b6-8084-67c66a597e9e">

#### ✅ In [3]
    all_files_data = []
    for file in all_files:
        data_frame = pd.read_excel(file)
        all_files_data.append(data_frame)
    all_files_data[0]
all_files에 저장된 데이터들을 순서대로 읽어서 all_files_data배열에 추가(.append)한다.

#### ☑️ Out [3]
<img width="907" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/bcaeb2bd-d9be-47e3-a517-dc48034372a1">

###### 💡실행할 때 <img width="395" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/11092d7c-0a9b-4797-9538-46ba609756b8">와 같은 에러가 발생할 수 있다. 관련 모듈이 미설치되서 뜨는 에러로 아래 모듈을 설치한다.
    !pip install xlrd

#### ✅ In [4]
    all_files_data_concat = pd.concat(all_files_data, axis=0, ignore_index=True)
.concat함수로 데이터 프레임의 행 열을 병합하여 all_files_data_concat에 저장한다.

#### ☑️ Out [4]
<img width="931" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/3b5999ee-e1f1-4c97-b09e-e3472855af15">

#### ✅ In [5]
    all_files_data_concat.to_csv('riss_bigdata.csv', encoding = 'utf-8', index = False)
.to_csv함수로 병합된 all_files_data_concat을 CSV파일로 저장한다.

#### ✅ In [6]
    all_title = all_files_data_concat['제목']
all_files_data_concat에서 제목만 추출한다.

#### ☑️ Out [6]
<img width="450" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/fae2a2df-8c24-4810-a50f-c249a5ef97bf">

#### ✅ In [7]
    sotpWords = set(stopwords.words("english"))
    lemma = WordNetLemmatizer()
1) stopwords.words(): 영어 불용어를 추출한다.
2) WordNetLemmatizer(): 표제어 추출 작업을 제공하는 객체를 생성한다.

#### ✅ In [8]
    words = []
    
    for title in all_title:
        EnWords = re.sub(r"[^a-zA-Z]+", " ", str(title))
        EnWordsToken = word_tokenize(EnWords.lower())
        EnWordsTokenStop = [w for w in EnWordsToken if w not in stopwords]
        EnWordsTokenStopLemma = [lemma.lemmatize(w) for w in EnWordsTokenStop]
        words.append(EnWordsTokenStopLemma)
1) EnWords: 알파벳으로 시작하지 않는 단어("[^a-zA-Z]+")를 공백으로 치환하여 제거re.sub()한다.
2) EnWordsToken: EnWords를 소문자(.lower())로 정규화 후 단어 토큰화(word_tokenize())를 한다.
3) EnWordsTokenStop: EnWordsToken에서 불용어(stopwords)를 제거한다.
4) EnWordsTokenStopLemma: EnWordsTokenStop에서 표제어(.lemmatize())를 추출한다.
5) 마지막으로 EnWordsTokenStopLemma를 words 배열에 추가(.append)한다.

#### ✅ In [9]
    print(words)
wodrs 배열 내용을 출력한다.

###### 💡In [9]를 실행하면 아래와 같은 에러가 발생할 수 있다.
<img width="397" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/47946f3c-5842-4d55-9a85-357be6b352e3">

###### nltk.download('stopwords')오류 이외에도 여러가지가 뜰 수 있는데 필요한 기능이 부족해서 그런것이기 때문에 모두 다운받아주면 된다.
    import nltk
    nltk.download('stopwords') //불용어 목록 다운
    nltk.download('punkt') //토근화를 위한 데이터 다운
    nltk.download('wordnet') //텍스트 분석 작업에서 단어의 의미 파악 및 단어 간의 관계 이해를 위해 다운

###### 💡In [9]를 실행하면 <img width="450" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/146845ae-9118-4b38-a7ce-ca9390668cd1">와 같은 에러가 발생할 수 있다. 이는 stopwords 변수가 WordListCorpusReader 형식의 객체인데 In [8]의 for문을 통한 반복이 불가능하기 때문에 발생하는 에러이다. stopwords 변수에는 불용어 데이터가 포함된 객체가 아니라 실제 불용어 데이터를 포함하는 리스트가 필요하기 때문에 리스트로 변환 후 실행하면 정상 출력된다.
###### In [7]을 아래와 같이 수정한다.
    stopwords_list = stopwords.words('english')
    lemma = WordNetLemmatizer()
###### In [8]에서 EnWordsTokenStop변수 아래와 같이 수정한다.
    EnWordsTokenStop = [w for w in EnWordsToken if w not in stopwords_list]

#### ☑️ Out [9]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/1d13afbb-0244-48e2-946a-64d03b3708ad">

#### ✅ In [10]
    words2 = list(reduce(lambda x, y: x+y, words))
    print(words2)
reduce()를 사용하여 words를 1차원 배열로 변환한다.

#### ☑️ Out [10]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/40d57d42-07e5-4642-8c4b-2f7d34ad485c">


### 🖥️데이터 탐색 및 분석 모델 구축
#### ✅ In [11]
    count = Counter(words2)
    print(count)
Counter()를 사용하여 words2 리스트에 있는 단어별로 출현 횟수를 계산한다.

#### ☑️ Out [11]
<img width="500" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/9ebf2cd6-f188-4847-a3ed-f0307085659b">

#### ✅ In [12]
    word_count = dict()
    
    for tag, counts in count.most_common(50):
        if(len(str(tag))>1):
            word_count[tag] = counts
            print("%s : %d" % (tag, counts))
1) count.most_common(50): count에서 가장 많이 출현한 상위 50개 단어를 리스트로 반환한다.
2) tag, counts: 반복문을 통해 1)의 결과를 tag에는 단어를 counts에는 해당 단어(tag)의 출현 횟수를 저장한다.
3) len(str(tag))>1: 단어(tag)의 길이가 1이상인 것만 다음 함수를 실행한다.
4) word_count[tag] = counts: 상단의 word_count 딕셔너리(dict())에 각 tag(단어)와 counts(등장 횟수)를 저장한다.

#### ☑️ Out [12]
<img width="200" alt="image" src="https://github.com/LeeSomgyul/Assignment_Bigdata/assets/140570847/98cf62ec-4c11-4854-b53c-1a3a966ba4db">

